apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: otel-collector
  namespace: monitoring
spec:
  dependsOn:
  - name: kube-prometheus-stack
    namespace: monitoring

  interval: 30m
  chart:
    spec:
      chart: opentelemetry-collector
      version: "0.x.x"
      sourceRef:
        kind: HelmRepository
        name: open-telemetry
        namespace: flux-system

  values:
    mode: deployment
    replicaCount: 1

    image:
      repository: otel/opentelemetry-collector-contrib

    presets:
      kubernetesAttributes:
        enabled: true

    service:
      enabled: true

    ports:
      otlp:
        enabled: true
        containerPort: 4317
        servicePort: 4317
      otlp-http:
        enabled: true
        containerPort: 4318
        servicePort: 4318
      metrics:
        enabled: true
        containerPort: 8888
        servicePort: 8888

    serviceMonitor:
      enabled: true
      additionalLabels: {}
      metricsEndpoints:
      - port: metrics
        interval: 30s
        path: /metrics

    prometheusRule:
      enabled: true
      groups:
      - name: opentelemetry-collector
        interval: 30s
        rules:
        # Processor alerts
        - alert: OTELProcessorDroppedSpans
          expr: sum(rate(otelcol_processor_dropped_spans[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: spans dropped by processor"
            description: |
              Some spans have been dropped by processor in {{ $labels.namespace }}/{{ $labels.pod }}
              Maybe collector has received non-standard spans or reached some limits
              Current rate: {{ $value }} spans/sec

        - alert: OTELProcessorDroppedMetrics
          expr: sum(rate(otelcol_processor_dropped_metric_points[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: metric points dropped by processor"
            description: |
              Some metric points have been dropped by processor in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} points/sec

        - alert: OTELProcessorDroppedLogs
          expr: sum(rate(otelcol_processor_dropped_log_records[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: log records dropped by processor"
            description: |
              Some log records have been dropped by processor in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} logs/sec

        # Receiver alerts
        - alert: OTELReceiverRefusedSpans
          expr: sum(rate(otelcol_receiver_refused_spans[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: spans refused by receiver"
            description: |
              Some spans have been refused by receiver in {{ $labels.namespace }}/{{ $labels.pod }}
              Transport: {{ $labels.transport }}, Receiver: {{ $labels.receiver }}
              Current rate: {{ $value }} spans/sec

        - alert: OTELReceiverRefusedMetrics
          expr: sum(rate(otelcol_receiver_refused_metric_points[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: metric points refused by receiver"
            description: |
              Some metric points have been refused by receiver in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} points/sec

        - alert: OTELReceiverRefusedLogs
          expr: sum(rate(otelcol_receiver_refused_log_records[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: log records refused by receiver"
            description: |
              Some log records have been refused by receiver in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} logs/sec

        # Exporter alerts
        - alert: OTELExporterEnqueueFailedSpans
          expr: sum(rate(otelcol_exporter_enqueue_failed_spans[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: spans failed to enqueue for export"
            description: |
              Some spans failed to enqueue in {{ $labels.namespace }}/{{ $labels.pod }}
              Exporter: {{ $labels.exporter }}
              Current rate: {{ $value }} spans/sec

        - alert: OTELExporterEnqueueFailedMetrics
          expr: sum(rate(otelcol_exporter_enqueue_failed_metric_points[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: metric points failed to enqueue for export"
            description: |
              Some metric points failed to enqueue in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} points/sec

        - alert: OTELExporterEnqueueFailedLogs
          expr: sum(rate(otelcol_exporter_enqueue_failed_log_records[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: log records failed to enqueue for export"
            description: |
              Some log records failed to enqueue in {{ $labels.namespace }}/{{ $labels.pod }}
              Current rate: {{ $value }} logs/sec

        - alert: OTELExporterSendFailedRequests
          expr: sum(rate(otelcol_exporter_send_failed_requests[1m])) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector: exporter send requests failed"
            description: |
              Some exporter requests failed in {{ $labels.namespace }}/{{ $labels.pod }}
              Exporter: {{ $labels.exporter }}
              Current rate: {{ $value }} requests/sec

        # Resource alerts
        - alert: OTELHighCPUUsage
          expr: max(rate(otelcol_process_cpu_seconds[1m])) * 100 > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "OTEL Collector: high CPU usage"
            description: |
              High CPU usage detected in {{ $labels.namespace }}/{{ $labels.pod }}
              Current CPU: {{ $value | humanizePercentage }}

        - alert: OTELHighMemoryUsage
          expr: |
            (otelcol_process_memory_rss / on(pod, namespace) 
            kube_pod_container_resource_limits{resource="memory", container=~".*otel.*"}) > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "OTEL Collector: high memory usage"
            description: |
              High memory usage detected in {{ $labels.namespace }}/{{ $labels.pod }}
              Current usage: {{ $value | humanizePercentage }} of limit

        - alert: OTELQueueAlmostFull
          expr: |
            (otelcol_exporter_queue_size / otelcol_exporter_queue_capacity) > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "OTEL Collector: export queue almost full"
            description: |
              Export queue is {{ $value | humanizePercentage }} full in {{ $labels.namespace }}/{{ $labels.pod }}
              Exporter: {{ $labels.exporter }}

        - alert: OTELCollectorDown
          expr: up{job="otel-collector"} == 0
          for: 3m
          labels:
            severity: critical
          annotations:
            summary: "OTEL Collector is down"
            description: |
              OTEL Collector instance {{ $labels.namespace }}/{{ $labels.pod }} is down

    config:
      extensions:
        health_check:
          endpoint: 0.0.0.0:13133

      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

      processors:
        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          # limit_mib: 256
          spike_limit_percentage: 25
        batch: {}

      exporters:
        debug:
          verbosity: detailed

        prometheus:
          endpoint: 0.0.0.0:8889

        otlp/jaeger:
          endpoint: jaeger-collector.monitoring.svc.cluster.local:4317
          tls:
            insecure: true

        otlphttp/loki:
          endpoint: http://loki.monitoring.svc.cluster.local:3100/otlp
          tls:
            insecure: true

      service:
        extensions: [ health_check ]
        pipelines:
          traces:
            receivers: [ otlp ]
            processors: [ memory_limiter, batch ]
            exporters: [ debug, otlp/jaeger ]
          metrics:
            receivers: [ otlp ]
            processors: [ memory_limiter, batch ]
            exporters: [ debug, prometheus ]
          logs:
            receivers: [ otlp ]
            processors: [ memory_limiter, batch ]
            exporters: [ debug, otlphttp/loki ]

        telemetry:
          resource:
            service.name: otel-collector
          metrics:
            level: detailed
            readers:
            - pull:
                exporter:
                  prometheus:
                    host: 0.0.0.0
                    port: 8888
                    with_resource_constant_labels:
                      included:
                      - service.name

    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
